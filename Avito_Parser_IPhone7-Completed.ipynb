{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Собираем общий парсер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт Библиотек\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Парсим ссылки на обьявления со страницы\n",
    "def get_list_page_html(soup):\n",
    "    list_page_html  = []\n",
    "    # Парсим ссылку конкретного обьявления\n",
    "    page = soup.findAll('a', class_ = 'snippet-link')\n",
    "\n",
    "    for i in page:\n",
    "        part_url = i.get('href')\n",
    "        # отбор обьявлений только по нужной нам теме\n",
    "        if 'iphone_7' in part_url:\n",
    "            full_url = url_head + part_url\n",
    "            list_page_html.append(full_url)\n",
    "            print(full_url)\n",
    "    return list_page_html\n",
    "\n",
    "# Получаем цену телефона по обьявлению\n",
    "def get_price (soup):\n",
    "    price = int(soup.find('span', class_ = 'js-item-price').get_text().replace(' ', ''))\n",
    "    #print(price)\n",
    "    return price\n",
    "\n",
    "# Получаем текст обьявления по обьявлению\n",
    "def get_text (soup):\n",
    "    text = soup.find('div', class_ = 'item-description-text').get_text().strip()\n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "# Получаем город обьявления по обьявлению\n",
    "def get_town (soup):\n",
    "    town = soup.find('span', class_ = 'item-address__string').get_text().strip()\n",
    "    #print(text)\n",
    "    return town"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Парсим по ссылке страницы с обьявлением данные обьявления\n",
    "# Ссылка на сайт который парсим\n",
    "#url = list_page_html[3]\n",
    "#print(url)\n",
    "# Получаем HTML страницу по ссылке\n",
    "def get_page_data(url):\n",
    "    page = requests.get(url)\n",
    "    # Получение из HTML страницы обьекта типа soup для разбора по HTML тегам\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    # Код 200 означает, что сайт откликнулся и выдал нам страницу\n",
    "    #print(page.status_code) \n",
    "    # Получаем город, цену и описание\n",
    "    # Формат записи данных в файл\n",
    "    data = {'town':get_town(soup), 'text': get_text(soup), 'price':get_price(soup)}\n",
    "    print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_towns = ['/moskva','/sankt-peterburg']#,'/rostov-na-donu','/krasnodar','/novosibirsk']\n",
    "url_head   = 'https://www.avito.ru'\n",
    "url_tail   = '/telefony/iphone-ASgBAgICAUSeAt4J?q=iphone+7&p='\n",
    "file_name  = './avito.csv'\n",
    "\n",
    "columns = ['town','text','price']\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "# Сборка полной ссылки для страницы с обьявлениями\n",
    "def get_page_url(url_head, town, url_tail, num_page):\n",
    "    full_url = url_head + town + url_tail + str(num_page)\n",
    "     \n",
    "#     print(town)\n",
    "#     print(url_tail)\n",
    "#     print(full_url)\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for cur_town in list_towns:\n",
    "    clear_output() # Очистка экрана    \n",
    "    for num_page in range(1,3):        \n",
    "        print(cur_town,' страница -', num_page)\n",
    "        # Ссылка на страницу обьявления с которой парсим\n",
    "        page_url = get_page_url(url_head, cur_town, url_tail, num_page)       \n",
    "        print(page_url)\n",
    "        try:\n",
    "            # Получаем HTML страницу по ссылке\n",
    "            page = requests.get(page_url)\n",
    "            # Получение из HTML страницы обьекта типа soup для разбора по HTML тегам\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "            # Код 200 означает, что сайт откликнулся и выдал нам страницу\n",
    "            print(page.status_code) \n",
    "            # Парсим ссылки на обьявления со страницы\n",
    "            list_page_html = get_list_page_html(soup)\n",
    "            print(list_page_html)\n",
    "            for cur_page_html in list_page_html:\n",
    "                try:\n",
    "                    print(cur_page_html)\n",
    "                    # заносим данные со страницы в датафрейм\n",
    "                    df.loc[len(df)] = get_page_data(cur_page_html)\n",
    "                    num += 1\n",
    "                    print('Спарсено обьявлений -', num)\n",
    "                except:\n",
    "                    print('Ошибка')\n",
    "                    pass\n",
    "            # Запись данных в файл\n",
    "            if len(df) > 0:\n",
    "                df.to_csv(file_name, mode='a', header = columns, index=False)\n",
    "                print('Сохранено успешно')\n",
    "                # Сброс данных в датафрейм\n",
    "            df = df.drop(df[:].index)\n",
    "        except:\n",
    "            #print(town,' страница -', num_page, ' не существует')\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
